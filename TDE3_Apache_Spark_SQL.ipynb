{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JasminiSantos/TDE3-Apache-Spark-SQL/blob/main/TDE3_Apache_Spark_SQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRDugJDwk8zU",
        "outputId": "f113882b-10f1-46bd-aa19-cdccd0e9e93c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=b071a2fe4dc17c43c029c1f0cd45bf46112c29f249b667ee4c6e28103241cbb0\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9Str360kKFE",
        "outputId": "921e3bb8-6ada-4405-b877-cbc2f29454ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-20 00:14:55--  https://jpbarddal.github.io/assets/data/bigdata/transactions_amostra.csv.zip\n",
            "Resolving jpbarddal.github.io (jpbarddal.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to jpbarddal.github.io (jpbarddal.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 47513871 (45M) [application/zip]\n",
            "Saving to: ‘transactions_amostra.csv.zip’\n",
            "\n",
            "transactions_amostr 100%[===================>]  45.31M   198MB/s    in 0.2s    \n",
            "\n",
            "2023-05-20 00:14:55 (198 MB/s) - ‘transactions_amostra.csv.zip’ saved [47513871/47513871]\n",
            "\n",
            "Archive:  transactions_amostra.csv.zip\n",
            "  inflating: transactions_amostra.csv  \n",
            "  inflating: __MACOSX/._transactions_amostra.csv  \n"
          ]
        }
      ],
      "source": [
        "!wget https://jpbarddal.github.io/assets/data/bigdata/transactions_amostra.csv.zip\n",
        "!unzip transactions_amostra.csv.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b94WpZgWkXze"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create SparkSession\n",
        "spark = SparkSession.builder\\\n",
        ".master('local[*]')\\\n",
        ".appName('tde3').getOrCreate()\n",
        "\n",
        "# Read the dataset into a DataFrame\n",
        "df = spark.read.csv(\"transactions_amostra.csv\", header=True, inferSchema=True, sep=\";\")\n",
        "\n",
        "# Register the DataFrame as a temporary view\n",
        "df.createOrReplaceTempView(\"transactions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtPsOyhPuUD1"
      },
      "outputs": [],
      "source": [
        "columns = df.take(1)[0].split(\";\")\n",
        "print(\"Columns of the dataset:\")\n",
        "for column in columns:\n",
        "    print(column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmkxqT8Bk-R8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "729d3563-f9ef-4fcf-a406-2a280d887848"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of transactions involving Brazil: 27463\n"
          ]
        }
      ],
      "source": [
        "# Problem 1: Number of transactions involving Brazil\n",
        "\n",
        "# Use Spark SQL to query the number of transactions involving Brazil\n",
        "transactions_in_brazil = spark.sql(\"SELECT COUNT(*) AS count FROM transactions WHERE country_or_area = 'Brazil'\")\n",
        "\n",
        "# Retrieve the count value\n",
        "count = transactions_in_brazil.first()[\"count\"]\n",
        "\n",
        "print(\"Number of transactions involving Brazil:\", count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nI1-DHdmlDKo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91fdb4b8-8154-44cb-ff0b-edd36c2a2803"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----+-----+\n",
            "|     Flow|Year|count|\n",
            "+---------+----+-----+\n",
            "|Re-Export|2006| 2431|\n",
            "|   Import|2009|31001|\n",
            "|   Import|2005|31124|\n",
            "|   Export|1995|11547|\n",
            "|Re-Export|1999| 2046|\n",
            "|Re-Import|2000|  952|\n",
            "|   Export|2000|16855|\n",
            "|   Export|2001|16759|\n",
            "|   Import|2015|28834|\n",
            "|Re-Export|1993|  973|\n",
            "|Re-Export|2013| 2225|\n",
            "|Re-Export|1998| 1519|\n",
            "|Re-Export|1997| 1605|\n",
            "|Re-Export|2016| 2298|\n",
            "|   Import|1998|24881|\n",
            "|   Export|1993| 7766|\n",
            "|Re-Export|1992|  764|\n",
            "|   Import|1991| 6185|\n",
            "|   Import|2011|31301|\n",
            "|   Import|1990| 4866|\n",
            "+---------+----+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Problem 2: Number of transactions per flow type and year\n",
        "\n",
        "# Calculate the number of transactions per flow and year\n",
        "transactions_per_flow_and_year = spark.sql(\"SELECT Flow, Year, COUNT(*) AS count FROM transactions GROUP BY Flow, Year\")\n",
        "\n",
        "# Show the results\n",
        "transactions_per_flow_and_year.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGOxGm1flGCH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ba8a0c2-82ef-43f5-9bbb-c5c3be61caa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------------------+\n",
            "|year|       average_value|\n",
            "+----+--------------------+\n",
            "|1990| 1.172426586778952E7|\n",
            "|2003|1.3028917611334749E7|\n",
            "|2007|2.3710673174875777E7|\n",
            "|2015|   3.1115574884196E7|\n",
            "|2006|2.1175872541099638E7|\n",
            "|2013| 3.306115128882995E7|\n",
            "|1997|   9549881.214776853|\n",
            "|1988| 1.864297055638571E7|\n",
            "|1994|1.1350325049077941E7|\n",
            "|2014| 4.612040441345007E7|\n",
            "|2004|1.5388487793083541E7|\n",
            "|1991| 1.306922385515173E7|\n",
            "|1996|1.1945524161286663E7|\n",
            "|1989|1.1263871329920229E7|\n",
            "|1998|1.0175610459598826E7|\n",
            "|2012|3.9028921881444596E7|\n",
            "|2009|2.5068409504465386E7|\n",
            "|2016| 2.941832757526777E7|\n",
            "|1995|1.2286454103356835E7|\n",
            "|2001|   9942220.288239626|\n",
            "+----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Problem 3: Average commodity values per year\n",
        "\n",
        "# Calculate average commodity values per year\n",
        "average_commodity_values = spark.sql(\"SELECT year, AVG(trade_usd) AS average_value FROM transactions GROUP BY year\")\n",
        "\n",
        "# Show the results\n",
        "average_commodity_values.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5WT5Z7xlI6u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d22d49f-3dfd-4636-8af6-4726ccb40571"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------------------+-------------------+\n",
            "|Year|            category|      average_price|\n",
            "+----+--------------------+-------------------+\n",
            "|2007|30_pharmaceutical...|            95057.0|\n",
            "|1993|27_mineral_fuels_...|          5313712.5|\n",
            "|2016|37_photographic_o...| 2319.6666666666665|\n",
            "|2000|     01_live_animals|          1952949.0|\n",
            "|1992|15_animal_vegetab...|          2108165.4|\n",
            "|2008|38_miscellaneous_...|       4.49661584E7|\n",
            "|2014|68_stone_plaster_...|          1.84138E7|\n",
            "|2006|27_mineral_fuels_...| 3407871.6666666665|\n",
            "|1993|28_inorganic_chem...|         4578559.24|\n",
            "|2008|28_inorganic_chem...|         4449268.25|\n",
            "|1994|39_plastics_and_a...| 3172772.5454545454|\n",
            "|2005|02_meat_and_edibl...|       2.09560584E8|\n",
            "|2007|48_paper_paperboa...|5.556047253333333E7|\n",
            "|1991|93_arms_and_ammun...|            69896.0|\n",
            "|1999|80_tin_and_articl...|        1.5815404E7|\n",
            "|1994|17_sugars_and_sug...|8.347317433333333E7|\n",
            "|1997|60_knitted_or_cro...|         3641387.75|\n",
            "|1999|22_beverages_spir...|          2038685.2|\n",
            "|1997|22_beverages_spir...|  663407.6666666666|\n",
            "|2007|35_albuminoids_mo...|        2.3422103E7|\n",
            "+----+--------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Problem 4: Average price of commodities per unit type, year, and category in the export flow in Brazil\n",
        "\n",
        "# Calculate average price per unit type, year, and category in export flow in Brazil\n",
        "average_price_per_unit_type_year_category = spark.sql(\"\"\"\n",
        "    SELECT Year, category, AVG(trade_usd) AS average_price\n",
        "    FROM transactions\n",
        "    WHERE Flow = 'Export' AND country_or_area = 'Brazil'\n",
        "    GROUP BY  Year, Category\n",
        "\"\"\")\n",
        "\n",
        "# Show the results\n",
        "average_price_per_unit_type_year_category.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXozkrG8mlVO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b93f3082-7015-460d-c52c-d15196c51c3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+----------------+\n",
            "|  Flow|           Commodity|  total_quantity|\n",
            "+------+--------------------+----------------+\n",
            "|Export|Iron ore, concent...|3.79546246752E11|\n",
            "+------+--------------------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Problem 5: Maximum, minimum, and mean transaction price per unit type and year\n",
        "transaction_stats_per_unit_type_and_year = df.groupBy(\"Unit\", \"Year\") \\\n",
        "    .agg({\"Price\": \"max\", \"Price\": \"min\", \"Price\": \"avg\"})\n",
        "transaction_stats_per_unit_type_and_year.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9w9YUp3QmoBw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ece586da-27d4-4da4-ee92-7d0b658199cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+--------------------+\n",
            "|country_or_area|           avg_price|\n",
            "+---------------+--------------------+\n",
            "|         Angola|1.636966606814285...|\n",
            "+---------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Problem 6: Country with the largest average commodity price in the Export flow\n",
        "\n",
        "# Execute the SQL query\n",
        "largest_avg_price_country = spark.sql(\"\"\"\n",
        "    SELECT country_or_area, AVG(trade_usd) AS avg_price\n",
        "    FROM transactions\n",
        "    WHERE Flow = 'Export'\n",
        "    GROUP BY country_or_area\n",
        "    ORDER BY avg_price DESC\n",
        "    LIMIT 1\n",
        "\"\"\")\n",
        "\n",
        "largest_avg_price_country.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jx8uhMRCmryN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "812f2aa1-1cda-4a16-c8b4-78382ea870d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+----------------+\n",
            "|  Flow|           Commodity|    total_amount|\n",
            "+------+--------------------+----------------+\n",
            "|Export|Iron ore, concent...|3.79546246752E11|\n",
            "+------+--------------------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Problem 7: Most commercialized commodity in 2016, per flow type\n",
        "\n",
        "# Execute the SQL query\n",
        "most_commercialized_commodity_2016 = spark.sql(\"\"\"\n",
        "    SELECT Flow, Commodity, SUM(quantity) AS total_amount\n",
        "    FROM transactions\n",
        "    WHERE Year = '2016'\n",
        "    GROUP BY Flow, Commodity\n",
        "    ORDER BY total_amount DESC\n",
        "    LIMIT 1\n",
        "\"\"\")\n",
        "most_commercialized_commodity_2016.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRvKsrPdmtqh"
      },
      "outputs": [],
      "source": [
        "# Stop the SparkSession\n",
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZIr4XltHkHUSfjUdAm7DO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}